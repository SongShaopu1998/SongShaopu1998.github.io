---
layout:		post
title:		MIT 6.S081
date:		2023-04-04
author:		shaopu
header-img:	img/code.png
catalog:	true

tags:
   - C
   - RISC-V
   - Operating System
---

在这篇文章中，我会记录一些容易遗忘的`OS`源码内容，以及所有`lab`的实验记录。

## xv6源码解析



## xv6实验记录

### MIT 6.S081 Lab1 util

在这里主要记录一下`pipe`相关的实验。

#### pingpong

首先，对于`pipe`的定义：

> A pipe is a small kernel buffer exposed to processes as apair of file descriptors, one for reading and one for writing. Writing data to one end of the peipe makes that data available for reading from theo other end of the pipe. Pipes process a way for processes to communicate.

再来看`pipe`系统调用的函数原型：

```C
int pipe(int p[])   // Create a pipe, put read/write file descriptors in p[0] and p[1].
```

我们可以用下图表示本函数中`pipe`的通讯过程：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-11-util-pingpong.jpeg" style="zoom: 33%;" />

当我们对一个进程`fork`之后，父进程和子进程都会有一份指向同一个`pipe`的文件描述符。

#### primes

![image-20230211153448293](https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-11-233448.png)

本题的挑战在于将`pipe`与递归结合起来。一个比较显而易见的事实是，在`main`函数中就需要`fork`进程，不能等到`primes_sieve`函数中再`fork`，这是因为主进程有着不同于其他进程的任务：他需要把`2~35`的所有数字通过`pipe`传递。

接下来思考在函数`primes_sieve`中需要做什么：

- 需要打印出当前作为判定参考的整数`n`

- 我们需要不断的从给定的`read_port`中读取数据（也即上一个进程中建立的`pipe`的`read_port`），并判断读出的数字是否不能被`n`整除，如果不能，就想办法传递到下一个进程中
- 为了将上一步筛选后的数字继续传递到下一个进程，我们需要在第一次遇到满足条件的数字后`fork`一个进程；并在函数一开始建立一个`pipe`用以传递数据

所以，函数的参数应该有两个：

1. 读取端口（因为该`pipe`是在上一个函数中建立的）
2. 判定参考数字（是从上一个进程筛选出来的第一个满足要求的数字）

```c
void
prime_sieve(int read_port, int n) {
  int start = n;
  fprintf(1, "prime %d\n", n);
  int p[2];
  pipe(p);
  uint st = 0;
  while (read(read_port, &n, 4)) {
    if (n % start != 0) {
      if (!st) {  // first time meet
        st = 1;
        if (fork() == 0) {
          close(p[1]);
          prime_sieve(p[0], n);
          // exit(0);
        }
      }
      write(p[1], &n, 4);
    }
  }
  close(p[1]);
  // must add this wait(0)
  wait(0);
  exit(0); 
}
```

本题需要使用`wait`来等待子进程结束后，主进程再结束。

### MIT 6.S081 Lab3 Page Tables

这个实验的后两个部分做了我好几天，中途几度做不下去了，上网搜了一圈发现大家的做法居然都是写一个将`user page table`复制到`kernele page table (per process)`的函数，然后分别在`exec`, `sbrk`等等函数内调用。但我的做法是在维护进程原有的用户页表的同时维护一个新的单进程内核页表，之后每一次更新`user page table`的时候也同时更新该页表。所以也只能硬着头皮继续做下去了，最终调试成功，把自己遇到的问题和分析记录一下。

#### 关于调试

首先初始化`home`文件夹下的`.gdbinit`文件（在macos系统下可以根据`gdb`的相关提示进行对应操作），之后使用命令`make qemu-gdb`启动调试，并在另一个命令行窗口输入`gdb-multiarch`开始调试，可以利用`layout split`命令在`gdb`窗口中查看当前调试位置和代码。如果要调试特定文件，则在开始调试时输入`file path+name`即可。

#### 关于内核页表

对于从`KERNBASE`到`PHYSTOP`这一段的区间以及下方的`memory-mapped device registers`，内核页表（`kernel_page_table`）会做`direct mapping`，也就是说如果探测到虚拟地址位于这段区间内，不需要通过页表转换才获得物理地址（在实际的`x86_64 linux`系统内也是这么设计的）。这也是为什么我们在程序中通过`PTE2PA`获得了所谓的“物理地址”（因为仍处于软件中，所以其实仍是一个虚拟地址）后，即使看起来内核页表中仍然没有对应的`VA->PA`的映射，但仍不会报错的原因。

#### 关于每个进程的页表设计

本部分分成两个子模块进行讨论：

- 原始的页表设计和当前的页表设计有何区别？我们都分别需要在什么时候切换页表？
- 为什么我们需要每个进程的内核页表？还有什么后续工作可以开展？

##### 原始设计

每个进程有一个`user space pagetable`。内核有唯一的`kernel pagetable`。我们在用户态和内核态切换时，需要切换 用户进程页表/内核页表，同时刷新`TLB`。我们可以查看`trampoline.S`文件中的`usertrap`和`userret`函数，其中包含了上述操作。

> 需要注意的是，此时我们不在`scheduler`，即调度器函数中切换页表，因为进程此时还处于内核态中，我们仍然需要使用`kernel pagetable`.

##### 改进设计

在本实验完成后，我们仍然需要在用户态和内核态切换时，切换页表。但同时，在调度器函数`scheduler`内部，在确定了即将运行的进程后，将**进程对应的内核页表**写入`satp`寄存器中，并刷新`TLB`。也就是说，即使在内核态中，当有进程运行时，我们要使用该进程对应的内核进程页表。

同时需要注意的是，在`exec`系统调用内部更新了进程内核页表后，也要重新写入`satp`寄存器并刷新`TLB`。因为原本的内核页表指针指向的位置可能已经被清空。

##### 为什么需要每个进程的内核页表？

在原始设计中，假如此时当我们在内核态中，传入了一个`user space address`时，因为此时的`kernel_page_table`中没有对应的`virtual address`的映射，就会报错。所以我们必须手动模拟把`VA`先转化成`PA`，再根据我们上一节讨论的内容得到硬件上的物理地址。

也就是说，此时寻址的方式是**利用软件模拟访问页表的过程获取物理地址**的。而现在我们想直接利用CPU的硬件寻址功能，即当前被储存在`satp`寄存器中的页表自动完成`VA->PA`的转换过程。也即实验指导中说的，可以直接在内核态中对传入的`user address pointer`解引用（解引用的过程隐含了从`VA->PA`的翻译过程），让系统自动完成翻译过程。

由此一来，在内核态中存储在`satp`寄存器中的页表就必须含有`VA->PA`的对应。

考虑到在原先的`kernel_page_table`中，`0-PLIC`的部分是没有被占用的，所以我们可以利用这一区域存储对应的`user memory mapping`。

> 关于内核页表中的`CLINT`，它位于`PLIC`的下方，但是在单独进程的内核页表中可以不做映射，也就是说在初始化的时候可以忽略这一区域。**否则在调试过程中会报`remap`这一区域的错误**。

除此之外，这次的实验还可以继续将每次内核态与用户态转换时需要的页表切换过程取消，这也是当前`linux`使用的减小开销的方法之一。

##### 调试出现的问题

- 单进程内核页表的映射函数的设计不能和`kvmmap`一样：`kvmmap`函数在系统`boot`时启用，所以如果出现内存分配不够的情况，就应当报`panic`。但是`kgtmap`是与`uvmcreate()`函数类似的使用方法，当内存耗尽时，应当返回-1而非报错，该方法设计不当将导致我们无法通过`execout`单元测试。同时，`kgtmap`函数在返回-1之前，应当释放掉已经分配的（刚创建的）页表：

  ```c
  int
  kpgtmap(pagetable_t pgt, uint64 va, uint64 pa, uint64 sz, int perm)
  {
    if (mappages(pgt, va, sz, pa, perm) != 0) {
      uvmfree_kernel_pages(pgt, 0);
      return -1;
    }
      
    return 0;
  }
  ```

  如果不及时释放页表，则会在单元测试的最后的空闲内存统计中检测到有内存页未被释放。

- 此外，在`vm.c`文件中，我们需要对原有的几个函数声明做改动：

  ```c
  void            uvminit(pagetable_t, pagetable_t, uchar *, uint);
  uint64          uvmalloc(pagetable_t, pagetable_t, uint64, uint64);
  uint64          uvmdealloc(pagetable_t, pagetable_t, uint64, uint64);
  int             uvmcopy(pagetable_t, pagetable_t, pagetable_t, pagetable_t, uint64);
  ```

  以便同时控制`proc_pagetable`和`kpagetable`。

  其中，`uvmalloc`和`uvmdealloc`，还有`uvmcopy`这三个函数格外需要注意，因为他们的内存分配很容易出现问题：需要注意用户页表需要销毁对应的物理页内存，但是单进程内核页表不需要销毁物理页内存。同时注意控制刚分配的内存的释放位置。

  为了方便起见，我将因`PLIC`的位置限制语句也写在`uvmalloc`中。

  ```c
  uint64
  uvmalloc(pagetable_t pagetable, pagetable_t kpagetable, uint64 oldsz, uint64 newsz)
  {
    char *mem;
    uint64 a;
  
    if(newsz < oldsz)
      return oldsz;
  
    oldsz = PGROUNDUP(oldsz);
    for(a = oldsz; a < newsz; a += PGSIZE){
      mem = kalloc();
      if(mem == 0){
        uvmdealloc(pagetable, kpagetable, a, oldsz);
        return 0;
      }    
      if (a >= PLIC) {
        kfree(mem);
        uvmdealloc(pagetable, kpagetable, a, oldsz);
        return 0;
      }
        
      memset(mem, 0, PGSIZE);
      //printf("start uvmalloc\n");
      if(mappages(pagetable, a, PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != 0){
        kfree(mem);
        uvmdealloc(pagetable, kpagetable, a, oldsz);
        return 0;
      }
      if (mappages(kpagetable, a, PGSIZE, (uint64)mem, PTE_W | PTE_X | PTE_R) != 0) {
        // since the operation of PGROUNDUP has happend above, no need to do it again
        int npages = (a - oldsz) / PGSIZE;
        // 不需要手动释放mem，因为该过程被包含在uvmunmap中
        uvmunmap(pagetable, a, npages + 1, 1);
        uvmunmap(kpagetable, a, npages, 0);
        return 0;
      }
    }
  
    return newsz;
  }
  ```

- 由于`kernel stack`的初始化在`proc.c/procinit()`函数中进行，我们在`allocproc()`和`exec()`函数中无需再分配内核栈的空间，但是需要对新创建的单进程内核页表进行内核栈的映射！我们可以直接利用`p->kstack`取得虚拟地址后，利用`kvmpa(va)`获取其物理地址：

  ```c
  // map the kernel stack
  uint64 va = p->kstack;
  if (kpgtmap(p->kpagetable, va, kvmpa(va), PGSIZE, PTE_R | PTE_W) != 0) {
    freeproc(p);
    release(&p->lock);
    return 0;
  }
  ```

- 在`exec`函数中，有一个关键点是：

  ```c
  ...
  proc_freepagetable(oldpagetable, oldsz);
  
  // MUST RELOAD THE KPAGE (UPDATE THE SATP)
  w_satp(MAKE_SATP(p->kpagetable));
  sfence_vma();
  
  // must put this under the above w_satp step
  uvmfree_kernel_pages(oldkpagetable, 0);
  if (p->pid == 1) vmprint(p->pagetable);
  return argc; // this ends up in a0, the first argument to main(argc, argv)
  ```

  我们必须先将新的单进程内核页表加载，之后再释放旧的页表内存。如果这里不更新`satp`寄存器或者先释放旧的页表内存，再更新`satp`寄存器，原本的页表地址内并没有页表存在，就会导致程序卡住。

### MIT 6.S081 lab4 Traps

这里为自己做个提醒：

1. 为了将`p->pretrapframe`的内容交给`p->trapframe`，由于两者的类型均是`trapframe*`，所以可以直接解引用：

   ```c
   *p->pretrapframe = *p->trapframe;
   ```

   但是需要注意的是，我们不能交换两个指针的位置，因为`trapframe`在`VA`中的地址是固定的，我们不能改变它。

2. 在`trap.c`中，我们应当通过改变`epc`的值来实现在`usertrap`函数结束后调用`handler`函数，之前我的写法是直接对报告的`p->handler`进行函数指针调用，这是一定不可以的，因为当前我们位于内核中，是不允许调用用户函数的。

3. 为了让程序在进入`handler`函数并返回后，仍然可以回到在中断处理之前对代码位置，我们需要新建一个`pretrapframe`来保存中断处理之前的状态，便于在调用`sigreturn()`时恢复原本的运行状态。

4. `stack frame`的结构：

   ![stack_frame](https://github.com/SongShaopu1998/MIT6.S081/blob/traps/stkf.png)

#### alarm是如何实现周期调用的？

首先通过`sigalarm`系统调用设置`alarm handler`，在`timer interrupt`返回后，虽然`yield`可能会让该进程暂时让出`CPU`，但当下一次系统执行该进程时，他会继续执行`trap`中的`usertrapret`，从而返回到我们设置的`trapframe->epc`的位置，并执行对应的`periodical()`函数（或者是我们自己定义的其他的什么函数）。

在`periodical()`函数结束时，我们会调用`sigreturn`。该函数主要负责两件事：

1. 让程序成功返回到执行`sigalarm`之前的位置（依靠上一节所说的`pretrapframe`）
2. 实现`periodical()`函数的周期调用

我们可以把上述过程用流程图表示一下：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-12-045428.jpg" alt="IMG_0502" style="zoom:33%;" />

#### 关于[timer interrupt](https://www.cnblogs.com/KatyuMarisaBlog/p/13934537.html)

本实验的基础是`xv6`的定时器中断。`risc-v`规定，定时器中断在`machine mode`下产生，且不可被屏蔽。这带来的一个问题就是，`timer interrupt`可能会破坏`CPU`任务的原子性，为了解决这个问题，`xv6`将定时器中断转化为`supervisor software interrupt`，交给`kernelvec`处理。

定时器中断的初始设置在`start.c`中，`timerinit`函数对`CLINT`（用于与定时器交互的`memory-mapped register`）编程，并将定时器中断的`trap handler`入口设置在`kernelvec`中的`tiemrvec`上，记录在`mtvec`寄存器上。同时准备一块`scratch`区域用于保存`timer interrupt handler`改变的寄存器等。

而在`timervec`函数内部，我们可以看到如下语句：

```asm
# kernelvec.S/timervec

li a1, 2
crsw sip, a1  # supervisor interrupt pending

...

mret  # return to user/supervisor mode
```

就是完成了我们将定时器中断交给`user/supervisor mode`下的软件中断处理的代码。当`mret`指令完成，进入`user/supervisor mode`后，程序根据当前是否允许中断来决定是否执行这个软件中断。也就是说，通过将不可屏蔽的定时器中断转化为软件中断，我们实现了其可屏蔽性，从而保证了某些场合下`CPU`操作的原子性。

### MIT 6.S081 lab5 Lazy Allocation

#### Part1

在更改`sbrk`函数后，执行`echo hi`会调出`usertrap`，根据提示中`sepc`的值，我们观察到其位于`sh.asm`文件中的`sd`指令，我们尝试向某个虚拟地址写入，但由于`sbrk`实际上没有分配物理内存并建立页表映射，导致当前页表找不到对应的`PA`翻译，所以会造成`trap`。

同时系统还会报出`panic: uvmunmap`的错误，通过在`gdb`中执行`bt`可以看到函数调用栈，是由于`wait`系统调用中的`free_proc`函数中调用了`uvmunmap`导致。观察`wait`源码可以发现，程序会对当前处于`ZOOMBIE`状态的进程进行内存销毁，而先前在`usertrap`函数中，进程会被设置为`killed`并调用系统函数`exit`来结束进程，并将进程状态设置为`ZOOMBIE`,这就会导致在`uvmunmap`中，检测到`PTE_V`没有设置，从而报`panic`的错误。

#### Part2

注意在`usertrap()`中处理虚拟地址时，必须对`r_stval()`的值进行`PGROUNDDOWN`的处理，因为只有这样我们才会在函数`mappages`中进行一次循环后立刻退出（`a`和`last`对应的是同一个地址），如果没有加上这个处理，则会分配额外的页内存。

#### Part3

在几个内核函数中，会有这个判断条件：

```c
    if((pte = walk(old, i, 0)) == 0)
      continue;
```

原本如果进入该`if`语句，程序会报`panic`，但是这里不需要，因为使用了`lazy allocation`之后，有可能我们连对应的`pagetable`多没有分配（只是将`size`标志扩大了），所以自然的在遇到该语句时可以直接`continue`.

- 在处理`fork`函数时，可能会遇到分配了`pagetable`但是没有分配`physical memory`的情况，此时我们可以再一次利用`walk`函数给新进程分配页表内存：

  ```c
  // some of the fork() code
  
  for(j = 0; j < p->sz; j += PGSIZE){
    if((pte = walk(p->pagetable, j, 0)) == 0)
      continue;
    if((*pte & PTE_V) == 0) {
       if (walk(np->pagetable, j, 1) == 0) continue;
    } else {
      pa = PTE2PA(*pte);
      flags = PTE_FLAGS(*pte);
      if((mem = kalloc()) == 0)
        goto err;
      memmove(mem, (char*)pa, PGSIZE);
      if(mappages(np->pagetable, j, PGSIZE, (uint64)mem, flags) != 0){
        kfree(mem);
        goto err;
      }      
    }
   }
  ```

- 这里最关键的一点在于处理`read/write`等`system call`调用中产生的`page fault`，因为在调用这些系统调用函数时就已经处于内核态中，所以此时发生的`page fault`会被`kerneltrap`捕获。我原本想在`kerneltrap`中利用`scause==13 || scause == 15`的判定条件处理`page fault`，但是后来发现，在内核态中出现的`page fault`并不被`riscv`定义为`page fault`，在`scause`寄存器的`trap code`列表中，内核态发生的最高位为1，并且没有单独的`page fault`对应，所以我们不能把处理函数写在`kerneltrap`中。

- 由于`read`和`write`函数调用`copyin/copyout`写入/读取内存，我们可以把相应的处理函数写在这两个函数中：

  ```c
  uint64 
  lazywalkaddr(pagetable_t pagetable, uint64 va)
  {
    struct proc *p = myproc();
  
    uint64 pa0;
    pa0 = walkaddr(pagetable, va);
    if(pa0 == 0) {
      // lazy allocation happens on the user stack region, but this function will be used in other cases besides the user stack
      if(va >= p->sz || va < PGROUNDDOWN(p->trapframe->sp))
        return 0;
      pa0 = (uint64)kalloc();
      if(pa0 == 0) {
        p->killed = 1;
      }
      memset((void *)pa0, 0, PGSIZE);
      if(mappages(pagetable, PGROUNDDOWN(va), PGSIZE, pa0, PTE_W|PTE_R|PTE_U) != 0){
        kfree((void *)pa0);
        p->killed = 1;
      }
    }
    return pa0;
  }
  ```

  需要注意，由于该函数除了会被用户栈区间的`lazy allocation`使用外，还用于其他的情况，所以不能在函数一开始就检测到`va >= p->sz || va < PGROUNDUP(p->trapframe->sp)`时立刻退出，而是应该在检测到`pa == 0`时，此时意味着我们要进行`lazy allocation`的处理了，再进行对虚拟地址范围的判断。
  
  > 在判定虚拟地址的合法边界时，`p->sz`指向了当前分配给进程内存的虚拟地址的顶部，在实际的操作系统中，实际上是增大了进程`heap`的边界（也是Linux中`sbrk()`的实际作用），`stack`区域的大小是固定的，这也是为什么即使我们使用的`p->trapframe->sp`是上一次陷入中断时保存的栈指针位置，在进行了`PGROUNDDOWN`操作后仍然可以得到栈的下边界的原因（栈指针是从高地址走向低地址的）。

### MIT 6.S081 lab6 Copy-On-Write

本题需要设计一个带锁的`refcount`结构。

- 在设置表明某一页是否是`COW`的比特位时，我们应该注意**被**fork的页也需要设置该位，原本我的做法是只在新页上设置`cow`，这意味着只有在向新页中写入时才会进行`cow copy`，但实际上如果我们向原本的进程指向的页中写入，也是需要复制的，不然其他的进程读取的数据就不是原本保存的那一份了。

- 在`trap`和`copyout`这两个函数中，为了改变处理页的引用计数，我原本调用的是`changerefcount`函数，但是由于可能的并发存在，如果两个进程同时进入了这两个函数，那么可能在经过两次`cow`之后，原本的物理页的引用计数就变成0了，所以我们必须调用`kfree`函数：

  ```c
  // cannot just call changerefcount
  kfree((void *)PGROUNDDOWN(pa0));
  ```

- 在`cow_check`函数中，我们应当注意`cornercase`，这会对结果的正确性产生很大影响：

  ```c
  // must have this line!!!
  if (pte == 0) return 0;
  ```

- 随时注意指针的解引用问题：包括当前指针是否为空，以及每一次指针指向的位置是否一样

- 在先前的`lazy allocation`实验中，我们需要对转换的地址范围作出限制，但是这里不需要，因为`lazy allocation`被限制在用户栈空间内，超出这个范围的地址如果要求`lazy allocation`会报错，但`cow`是基于`fork`的，`fork`的工作原理确保了我们不会在地址范围内做出不合法的转换操作。

#### 为什么`COW`只需要对`copyout`做更改？而不需要更改`copyin`? 

`copyin`和`copyout`的本质区别是，`copyin`是利用`VA->PA`转化得到的地址中读取，而`copyout`则是利用`VA->PA`转化得到的地址写入。由于在`fork`之后我们将父进程和子进程的`PTE_W`都设置成了`0`，从对应地址中读取数据并不会有任何问题，我们要处理的是向对应地址写入数据的情况。所以`copyin`这里不需要做额外的特殊处理。

在上一个实验中，我们对`copyin`和`copyout`都做了`page fault handler`的原因在于，因为我们根本没有为刚分配的那块内存区域建立任何页表映射，在这两个函数内部`walkaddr`的结果就是返回`0`，所以都需要做额外处理。

### MIT 6.S081 Lab7 Multithreading

本次试验分为三部分：

- 建立一个简易的`user-level multithreading`机制
- 使用`linux`的`pthread_create`, `pthread_join`以及`pthread_mutex_lock`
- 使用`linux`的 `pthread_mutex`和`pthread_cond`（条件变量）建立`barrier`机制

下边逐一对本实验进行分析。

#### user-level Multithreading

这部分的实验基本上是模仿了`linux`中线程创建与使用的API。其中的线程数据结构包含了线程栈，线程状态，以及对应的线程当前的执行状态`context`：

```c
struct thread {
  char       stack[STACK_SIZE]; /* the thread's stack */
  int        state;             /* FREE, RUNNING, RUNNABLE */
  struct context context;
};
struct thread all_thread[MAX_THREAD]; // thread pool
```

`thread_create`函数接受一个函数指针，并将`context`中对应的`ra`设置成给入的函数指针地址，`sp`设置成当前线程栈的栈顶位置。这样当下次执行`thread_schedule`时，就会在`thread_switch`中将`context->ra`和`context->sp`给入到系统当前的`ra`和`sp`内。

#### Using threads

这部分的实验使用多线程操作一个哈希表，为了减少因锁带来的等待时间，我们给每一个`bucket`分配一个`pthread_mutex`，我们可以看到`linux`自身的`pthreads_create`和上一部分我们定义的`therad_create`是十分类似的。对于`pthread_mutex`，对应的上锁和解锁操作分别是：

```c
pthread_mutex_init(&lock[i]); // init before using
pthread_mutex_lock(&lock[i]);
pthread_mutex_unlock(&lock[i]);
```

#### Barrier

> In [parallel computing](https://en.wikipedia.org/wiki/Parallel_computing), a **barrier** is a type of [synchronization](https://en.wikipedia.org/wiki/Synchronization_(computer_science)) method. A barrier for a group of threads or processes in the source code means any thread/process must stop at this point and cannot proceed until all other threads/processes reach this barrier.

在本部分的实验中，我们要完成`barrier`函数。这里必须结合**条件变量**使用才可以。如果没有**条件变量**，由于在进入`barrier`函数时当前线程必须先获取一把锁`bstate.barrier_mutex`，并在函数结束时将锁释放掉；我们无法避免如下的问题：

>  Make sure that a thread that leaves the barrier and races around the loop doesn't increase `bstate.nthread` while a previous round is still using it.

可能这个线程在本轮已经被使用过了，但是由于在最后我们释放了`barrier`上的锁，那么可能不久之后同样的线程又拿到了CPU，但这对于他来讲已经是下一个`round`了；但与此同时其他的线程还在本`round`没有结束，这显然是不符合题目要求的。

所以通过使用`pthread_cond_wait(&bstate.barrier_cond, &bstate.barrier_mutex)`，当前线程释放互斥锁，同时`sleep`在`barrier_cond`上。这样一来，其他的线程就可以进入`barrier()`函数内部执行本轮操作，当我们发现所有的线程都执行完了这一轮后（`++bstate.nthread == nthread`），通过广播（`pthread_cond_broadcast(&bstate.barrier_cond)`）唤醒所有`sleep`在条件变量上的线程，让他们`move forward`。进入下一轮。

### MIT 6.S081 lab8: locks

在讨论本次实验的具体设计之前，我打算先记录下`xv6`中关于`spinlock`和`sleeplock`的相关设计。

#### spinlock

自旋锁的结构如下所示：

```c
// Mutual exclusion lock.
struct spinlock {
  uint locked;       // Is the lock held?

  // For debugging:
  char *name;        // Name of lock.
  struct cpu *cpu;   // The cpu holding the lock.
};
```

原始的`spinlock`设计意图：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-13-011256.png" alt="image-20230212171256228" style="zoom:50%;" />

为了避免如上图所示的问题，`acquire`和`release`被设计成如下的样子，由于代码中已经给出了很详细的注释，这里就不再赘述：

```c
// Acquire the lock.
// Loops (spins) until the lock is acquired.
void
acquire(struct spinlock *lk)
{
  push_off(); // disable interrupts to avoid deadlock.
  if(holding(lk))
    panic("acquire");    

  // On RISC-V, sync_lock_test_and_set turns into an atomic swap:
  //   a5 = 1
  //   s1 = &lk->locked
  //   amoswap.w.aq a5, a5, (s1)
  while(__sync_lock_test_and_set(&lk->locked, 1) != 0) {
   ;
  }

  // Tell the C compiler and the processor to not move loads or stores
  // past this point, to ensure that the critical section's memory
  // references happen strictly after the lock is acquired.
  // On RISC-V, this emits a fence instruction.
  __sync_synchronize();

  // Record info about lock acquisition for holding() and debugging.
  lk->cpu = mycpu();
}

// Release the lock.
void
release(struct spinlock *lk)
{
  if(!holding(lk))
    panic("release");

  lk->cpu = 0;

  // Tell the C compiler and the CPU to not move loads or stores
  // past this point, to ensure that all the stores in the critical
  // section are visible to other CPUs before the lock is released,
  // and that loads in the critical section occur strictly before
  // the lock is released.
  // On RISC-V, this emits a fence instruction.
  __sync_synchronize();

  // Release the lock, equivalent to lk->locked = 0.
  // This code doesn't use a C assignment, since the C standard
  // implies that an assignment might be implemented with
  // multiple store instructions.
  // On RISC-V, sync_lock_release turns into an atomic swap:
  //   s1 = &lk->locked
  //   amoswap.w zero, zero, (s1)
  __sync_lock_release(&lk->locked);

  pop_off();
}
```

#### sleeplock

本部分可以在`xv6`的课本的`7.5`节内容找到，这里记录一下比较关键的部分。

作者以信号（`semaphore`）为例，使用生产者-消费者模型来说明`sleep-wakeup`（`sequence coordination/conditional synchronization mechanisms`）:

如果我们简单的使用`spinlock`，呈现出的程序如下：

```c
struct semaphore {
  struct spinlock lock;
  int count;
};

void
V (struct semaphore *s) // producer
{
  acquire(&s->lock);
  s->count += 1;
  release(&s->lock);
}

void
P (struct semaphore *s) //consumer
{
  while (s->count == 0)
    ;
  acquire(&s->lock);
  s->count -= 1;
  release(&s->lock);
}
```

这种写法没有问题，但是`consumer`所在的`CPU`会花费大量时间轮询（`polling`）信号对应的`count`，浪费资源。为了让出当前他使用的`CPU`，我们想到如下的办法：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-13-015307.png" alt="image-20230212175307165" style="zoom: 25%;" />

> 这里使用`while`的原因是，即使在线程被唤醒后，也要检查此时的`s->count`是否为`0`，如果还是`0`，那么要继续`sleep`.（预防`supurious wakeup`）

但这种写法会引发`lost wake-up`问题，即：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-13-015804.png" alt="image-20230212175804428" style="zoom:50%;" />

为了解决`lost wake-up`问题，我们试图将`P`中的获取锁的操作移到`while`之前，但是这会导致`sleep`之后，程序死锁，所以我们要对`sleep`操作进行一些更改：

> The caller must pass the condition lock to sleep so it can release the lock after the calling process is marked as asleep and waiting on the sleep channel. The lock will force a concurrent V to wait until P has finished putting itself to sleep, so that the wakeup will find the sleeping consumer and wake it up. Once the consumer is awake again sleep reacquires the lock before returning.

```c
void
acquiresleep(struct sleeplock *lk)
{
  acquire(&lk->lk);
  while (lk->locked) {
    sleep(lk, &lk->lk);
  }
  lk->locked = 1;
  lk->pid = myproc()->pid;
  release(&lk->lk);
}

void
releasesleep(struct sleeplock *lk)
{
  acquire(&lk->lk);
  lk->locked = 0;
  lk->pid = 0;
  wakeup(lk);
  release(&lk->lk);
}
```

而内部的`sleep`与`wakeup`的代码如下所示：

```c
// Atomically release lock and sleep on chan.
// Reacquires lock when awakened.
void
sleep(void *chan, struct spinlock *lk)
{
  struct proc *p = myproc();
  
  // Must acquire p->lock in order to
  // change p->state and then call sched.
  // Once we hold p->lock, we can be
  // guaranteed that we won't miss any wakeup
  // (wakeup locks p->lock),
  // so it's okay to release lk.
  if(lk != &p->lock){  //DOC: sleeplock0
    acquire(&p->lock);  //DOC: sleeplock1
    release(lk);
  }

  // Go to sleep.
  p->chan = chan;
  p->state = SLEEPING;

  sched();

  // Tidy up.
  p->chan = 0;

  // Reacquire original lock.
  if(lk != &p->lock){
    release(&p->lock);
    acquire(lk);
  }
}

// Wake up all processes sleeping on chan.
// Must be called without any p->lock.
void
wakeup(void *chan)
{
  struct proc *p;

  for(p = proc; p < &proc[NPROC]; p++) {
    acquire(&p->lock);
    if(p->state == SLEEPING && p->chan == chan) {
      p->state = RUNNABLE;
    }
    release(&p->lock);
  }
}
```

这种`sleep/wakeup`机制用在很多地方，比如`piperead/pipewrite`就是一个很巧妙的例子，利用`struct pipe`中的`nread`和`nwrite`作为`sleep`的`channel`,我们可以在`pipe.c`文件中查看。下边主要记录比较重要的系统调用：

#### wait

```c
// Wait for a child process to exit and return its pid.
// Return -1 if this process has no children.
int
wait(uint64 addr)
{
  struct proc *np;
  int havekids, pid;
  struct proc *p = myproc();

  // hold p->lock for the whole time to avoid lost
  // wakeups from a child's exit().
  acquire(&p->lock);

  for(;;){
    // Scan through table looking for exited children.
    havekids = 0;
    for(np = proc; np < &proc[NPROC]; np++){
      // this code uses np->parent without holding np->lock.
      // acquiring the lock first would cause a deadlock,
      // since np might be an ancestor, and we already hold p->lock.
      // we must keep the same locking order: locking the parent first, then its child to avoid deadlock
      if(np->parent == p){
        // np->parent can't change between the check and the acquire()
        // because only the parent changes it, and we're the parent.
        acquire(&np->lock);
        havekids = 1;
        if(np->state == ZOMBIE){
          // Found one.
          pid = np->pid;
          if(addr != 0 && copyout(p->pagetable, addr, (char *)&np->xstate,
                                  sizeof(np->xstate)) < 0) {
            release(&np->lock);
            release(&p->lock);
            return -1;
          }
          freeproc(np);
          release(&np->lock);
          release(&p->lock);
          return pid;
        }
        release(&np->lock);
      }
    }

    // No point waiting if we don't have any children.
    if(!havekids || p->killed){
      release(&p->lock);
      return -1;
    }
    
    // Wait for a child to exit.
    sleep(p, &p->lock);  //DOC: wait-sleep
  }
}
```

由于`wait`使用的`conditional lock`是`p->lock`，所以在`sleep`的代码中需要做如下的判断来避免`deadlock`:

```c
if(lk != &p->lock){  //DOC: sleeplock0
  acquire(&p->lock);  //DOC: sleeplock1
  release(lk);
}
```



#### Memory Allocator

本部分的实验需要我们通过给每一个`CPU`核保存一份独立的`freelist`的方式提高系统并发度。

> The basic idea is to maintain a free list per CPU, each list with its own lock. Allocations and frees on different CPUs can run in parallel, because each CPU will operate on a different list. The main challenge will be to deal with the case in which one CPU's free list is empty, but another CPU's list has free memory; in that case, the one CPU must "steal" part of the other CPU's free list.

```c
// one freelist per cpu
struct {
  struct spinlock lock;
  struct run *freelist;
} kmem[NCPU];
```

#### Buffer Cache

在将`buffer cache`设计成哈希表的形式后，在数据结构中`bcache`有两个锁：

- 哈希表中的每个`bucket`都有一个`spinlock`，用来保护该`bucket`中的所有`metadata`
- 每个`buf`都有一个`sleeplock`，用来锁住`buffer`中用来存储和读取数据的`data`数组。

至于为什么要这样设计，个人觉得是为了提高操作的解耦度，允许更多的并行操作，比如我们在更改某一个`buffer`中的`data[]`时就不需要给整个的`bucket`加锁。

这里与先前的设计不同的是，在之前的`bget`函数中，为了找到合适的`buffer cache`分配，在`struct buf`内部维护了一个`LRU Cache list`，每次通过对链表的操作维护时序性。但是在这里，我们直接利用`CPU ticks`，每次只要更新使用时的`ticks`就可以了，需要`evict`时，从空闲的`entry`中选出`least recently used`的那一个。

### MIT 6.S081 lab9 file system

在对本次实验的具体内容分析之前，先记录一下`file system`中关键的部分：

文件系统的层次架构如下所示：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-13-094107.png" alt="image-20230213014107087" style="zoom:50%;" />

其中，`buffer cache`在上一个实验中已经有所接触，这里主要关注从`Inode`到`FIle descriptor`的部分。而`Logging`作为单独的主题，这里不再探讨。

`xv6`将磁盘分成如下的组织结构：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-13-101440.png" alt="image-20230213021440733" style="zoom:50%;" />

其中，`superblock`包含了文件系统的`metadata`:

```c
// Disk layout:
// [ boot block | super block | log | inode blocks |
//                                          free bit map | data blocks]
//
// mkfs computes the super block and builds an initial file system. The
// super block describes the disk layout:
struct superblock {
  uint magic;        // Must be FSMAGIC
  uint size;         // Size of file system image (blocks)
  uint nblocks;      // Number of data blocks
  uint ninodes;      // Number of inodes.
  uint nlog;         // Number of log blocks
  uint logstart;     // Block number of first log block
  uint inodestart;   // Block number of first inode block
  uint bmapstart;    // Block number of first free map block
};
```

如果想要得到某个`inode`所在的`block`或者`bitmap` 所在的`block`，只需要经过一些简单的宏定义换算即可：

```c
// Block containing inode i
#define IBLOCK(i, sb)     ((i) / IPB + sb.inodestart)

// Bitmap bits per block
#define BPB           (BSIZE*8)

// Block of free map containing bit for block b
#define BBLOCK(b, sb) ((b)/BPB + sb.bmapstart)

...
```

`inode`是我们比较关心的部分，它在磁盘和内存中都记录了文件的相关信息：

```c
// in-memory copy of an inode
struct inode {
  uint dev;           // Device number
  uint inum;          // Inode number
  int ref;            // Reference count
  struct sleeplock lock; // protects everything below here
  int valid;          // inode has been read from disk?

  short type;         // copy of disk inode
  short major;
  short minor;
  short nlink;
  uint size;
  uint addrs[NDIRECT+2];
};

// On-disk inode structure
struct dinode {
  short type;           // File type
  short major;          // Major device number (T_DEVICE only)
  short minor;          // Minor device number (T_DEVICE only)
  short nlink;          // Number of links to inode in file system
  uint size;            // Size of file (bytes)
  uint addrs[NDIRECT+2];   // Data block addresses
};
```

这里的`inode`和`dinode`的定义已经根据本次试验的要求做了更改，因为我们需要定义一个`doubly-indirect block`，最后记录`data block address`的`addrs`数组有所更改。对于只支持`single-indirect block`的`inode`结构如下：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-02-13-232749.png" alt="image-20230213152748749" style="zoom:50%;" />

在`inode`的数据结构中，`ref`和`nlink`比较容易混淆：

>-  The `ref` field counts the number of **C pointers** referring to the in-memory inode, and the kernel discards the `inode` from memory if the reference count drops to zero.
>- The `nlink` field (on disk and copied in memory if it is cached) that counts the number of **directory entries** that refer to a file; xv6 won’t free an `inode` if its link count is greater than zero.

也就是说，`ref`和`nlink`本质上都是`lock-like mechanism`，当他们当中有任何一个不为`0`的时候，`inode`都不会被释放。

需要注意的是，`directory`只是一种特殊的`inode`罢了，如果我们查看`dirlink`的代码就会发现，它只不过是将`directory entry`，即`dirent`的结构通过调用`writei`写入指定`inode`的`buffer cache block`中而已。

```c
struct dirent {
  ushort inum;  // the corresponding inode number
  char name[DIRSIZ];
};
```

而最后，文件描述符（`file descriptor`）提供了最顶层的抽象：

```c
struct file {
  enum { FD_NONE, FD_PIPE, FD_INODE, FD_DEVICE } type;
  int ref; // reference count
  char readable;
  char writable;
  struct pipe *pipe; // FD_PIPE
  struct inode *ip;  // FD_INODE and FD_DEVICE
  uint off;          // FD_INODE
  short major;       // FD_DEVICE
};
```

文件系统里的`bitmap`是一个很有趣的数据结构，我们可以通过如下分配新的`disk block`的代码感受它是如何工作的：

```c
// Allocate a zeroed disk block.
static uint
balloc(uint dev)
{
  int b, bi, m;
  struct buf *bp;

  bp = 0;
  for(b = 0; b < sb.size; b += BPB){
    bp = bread(dev, BBLOCK(b, sb));
    for(bi = 0; bi < BPB && b + bi < sb.size; bi++){
      m = 1 << (bi % 8);
      if((bp->data[bi/8] & m) == 0){  // Is block free?
        bp->data[bi/8] |= m;  // Mark block in use.
        log_write(bp);
        brelse(bp);
        bzero(dev, b + bi);
        return b + bi;
      }
    }
    brelse(bp);
  }
  panic("balloc: out of blocks");
}
```

在这份代码中，我们可以看到程序是按照有多个`bitmap`的假设处理的，虽然在该系统中，我们只安排了一个`bitmap`。该函数最后返回我们分配的`block number`。

除了关注这几个基本的数据结构外，这里再看一下一个比较重要的辅助函数`create`，该函数在许多场合被调用，包括`sys_open`中创建文件时，我们可以看到它是如何把文件名和文件内容本身联系起来的：

```c
static struct inode*
create(char *path, short type, short major, short minor)
{
  struct inode *ip, *dp;
  char name[DIRSIZ];
  if((dp = nameiparent(path, name)) == 0)
    return 0;
  ilock(dp);
  if((ip = dirlookup(dp, name, 0)) != 0){
    // ...
  }
  
  // ...
  
  if(dirlink(dp, name, ip->inum) < 0)
    panic("create: dirlink");

  iunlockput(dp);

  return ip;
}

// Write a new directory entry (name, inum) into the directory dp.
int
dirlink(struct inode *dp, char *name, uint inum)
{
  int off;
  struct dirent de;
  struct inode *ip;

  // Check that name is not present.
  if((ip = dirlookup(dp, name, 0)) != 0){
    iput(ip);
    return -1;
  }

  // Look for an empty dirent.
  for(off = 0; off < dp->size; off += sizeof(de)){
    if(readi(dp, 0, (uint64)&de, off, sizeof(de)) != sizeof(de))
      panic("dirlink read");
    if(de.inum == 0)
      break;
  }

  strncpy(de.name, name, DIRSIZ);
  de.inum = inum;
  if(writei(dp, 0, (uint64)&de, off, sizeof(de)) != sizeof(de))
    panic("dirlink");

  return 0;
}
```

利用`dirlink`函数，`create`将文件名和其对应的`inode`编号存储在一个`dirent`结构中，作为一个`directory entry`将其链接到上级目录的`inode(directory)`的`buffer->data`中。

下边我们来看一下这次的实验内容。

#### Larger files

这部分的实验要求我们更改`bmap`以及对应的`itrunc`。`bmap(struct inode *ip, uint bn)`函数的作用顾名思义，就是获取`ip`的第`bn`个`block`。函数的实现思路就是分情况讨论:

- Direct block
- Single-indirect block
- Double-indirect block

如果我们发现对应的地址中存储的值为`0`，则利用`balloc()`分配一个新的`block`并将对应地址写入。

#### Symbolic links

首先我们要搞清楚硬连接（`hrad link`）和软连接（`soft link/symbolic link`）的区别。

硬连接通过系统调用`sys_link/sys_unlink`建立/撤销，对`inode->nlink`进行增减；本质上是给当前的文件一个别名，具体点说是在一个新的`directory entry`上建立当前`inode`的一个别名。

硬连接必须和源文件建立在同一个磁盘上，因为`inode number`只有在同一块磁盘上才有意义。

软连接可以与原文件不在一个磁盘上，同时在创建软连接时，源文件可以不存在（查看`sys_symlink`的代码就很容易知道为什么）

而软连接则是利用`create`新建了一个`inode`后，将对应的`oldlink`存储到这个新建的`inode`的`data buffer`里：

```c
uint64
sys_symlink(void)
{
  // ...
  if ((ip = create(path, T_SYMLINK, 0, 0)) == 0) {
    end_op();
    return -1;    
  }

  // lock the inode
  if (writei(ip, 0, (uint64)target, 0, length) < length) {
    end_op();
    return -1; 
  }
  // ...
}
```

之后，我们要修改`sys_open`，当检测到是一个`T_SYMLINK`类型的文件时，如果没有指定`NOFOLLOW`，我们就一直跟踪存储在对应`inode`的`buffer`的文件对应的`inode`（相当于一个递归的过程），直到：

1. 探查深度超过阈值，此时我们判定为`cycle-link`
2. 新的文件名指向的`inode`类型不是`T_SYMLINK`

### MIT 6.S081 lab10 mmap

在本实验中，我们实现了一个简易版本的`mmap/munmap`系统调用。

因为`mmap`和`munmap`是与文件操作相关的，所以我将函数定义在`sysfile.c`文件中。

对于`mmap`，按照步骤，有如下的注意事项：

- 检查当`flags`是`MAP_SHARED`时（意味着我们当前在`RAM`中做的改动最后都要映射回原本的文件中），我们不能对以只读模式（文件描述符对应的`readable`位为`0`）打开的文件写入（`perm`不能为`PROT_WRITE`）。
- 在进程对应的虚拟内存中找到一块空闲的区域，分配给即将存储的文件内容，并在定义的`VMA(Virtual Memory Area)`的数据结构中记录下对应信息。

```c
struct VMA {
  struct file* fd;
  int pid;
  uint64 curvend;  // the -current- vma end point
  uint64 vend;    // the initial allocated vma end point
  uint curlength; // the -current- length of the vma region (which will be changed over time with munmap call)
  uint length;    // the initial length of vma region
  int perm;       // the permission of vma given by mmap call (PROT_READ & PROT_WRITE)
  int offset;
  int flags;      // either be MAP_SHARED or MAP_PRIVATE
  int mapped;     // use the binary representation indicating the -current- mapped pages
};
```

在这一步时，我们可以比较聪明的每次都将空闲内存分配到`p->sz`以上的区域中（也即当前`heap`的顶部上方的内存区域），同时把对应的`p->sz`更新成分配之后的顶端位置。这样做的好处在于，由于在`mmap`中我们使用了`lazy allocation`，如果在真实写入内容之前，该进程执行其他任务也分配了一些内存，我们也不会把原本分配给`mmap`的内存块占用掉。

- 使用`filedup`增加打开对应文件的引用计数。

`lazy allocation`的部分按照惯例仍然写在`usertrap`中：

- 由于同一个进程可能分配多个`VMA`，在`VMA`当中除了记录进程对应的`pid`之外，也要记录分配的地址信息，从而通过`va`和`pid`确定当前我们要处理的`VMA`是哪一个。
- 分配内存并映射到页表上，再用`readi`将文件内容读入内存，需要注意我们此时不能确保`readi`的返回值就一定是`PGSIZE`，因为文件大小未必是`PGSIZE`的整数倍。
- 为了记录在我们预留出的`N * PGSIZE`大小的区域上，哪几页真的存上了内容，我是用位操作，用一个整数`mapped`记录哪几页做了映射：

```c
vma->mapped += (1L << (off / PGSIZE));
```

`munmap`的操作要相应的复杂一些，具体的操作过程详见代码，但大概思路就是每次只将做了映射的部分`unmap`掉，同时如果有`MAP_SHARED & PROT_WRITE`的标记，使用`filewrite`写回文件中。每一次`unmap`之后，我们都会更新当前分配内存区域的`end point`以及`length`.

如果我们发现要`unmap`的长度等于先前一开始`map`的长度，就清空对应的`vma`内容，并将文件的引用计数减一。

最后，在系统调用`fork`中记得复制一份`vma`的内容，在`exit`中尝试找到所有属于该进程的`vma`，进行`unmap`和清空。
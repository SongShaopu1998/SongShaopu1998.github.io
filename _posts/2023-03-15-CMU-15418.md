---
layout:		post
title:		CMU 15-418
date:		2023-03-15
author:		shaopu
header-img:	img/code.png
catalog:	true

tags:
   - C++
   - Parallel Computing
---

本文写在`61C`的并行计算部分后。

## Lecture 2: A Modern Multi-Core Processor

这节课的内容可以大致分成两部分。首先讲述了如何提高峰值并行效率，其次讲解了如何缓解/减少由于内存访问带来的并行效率下降的问题。

### Coherent execution

`SIMD`指令与`multi-core`不同，`SIMD`指令运行的指令一定都是一样的（只是输入不一样），但多核可以并行运行完全不同的指令，所以在`SIMD`遇到分支时就会产生一些问题。

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-055606.png" alt="image-20230315225606232" style="zoom: 33%;" />

如果循环体内部含有分支语句，意味着对于不同的输入，并不能够在一开始就确定走`if`还是`else`，所以`SIMD`指令不能直接利用。此时我们可以将指令重排列，从而将走`if`的语句安排在一起，走`else`的语句安排在一起。而如果不想重排指令，我们就要使用如上图所示的方法，即分别对`if`和`else`做一次`SIMD`运算，然后根据输入设定`mask`，来决定到底将哪些对应的计算结果输出。我们能够预想到的最坏情况发生在两个分支的代码复杂度相差较大时，并且此时绝大多数数据都走向了复杂度较小的分支。那么因为对于两个分支都要进行一遍`SIMD`运算，就会导致我们在无用的运算结果上花费了大量时间。换句话说，`SIMD`的高效运算取决于避免上述情况的发生，我们将这种先决条件称为`Coherent execution`。

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-060510.png" alt="image-20230315230510091" style="zoom:33%;" />

### summary of parallel execution

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-061117.png" alt="image-20230315231117483" style="zoom:33%;" />

我们有三种并行运算的思路：

- **Multi-core** (thread-level)
- **SIMD** (instruction level)
- **Superscalar**

> 其中`Superscalar`着重在**一个**内核中对**一个**`instruction stream`执行多个独立指令的并行运算

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-061147.png" alt="image-20230315231144290" style="zoom:33%;" />

需要注意的是，`SIMD`指令是比“多线程”更加底层的技法，他的指令执行由计算单元完成，只需要一个线程就可以执行，并非需要多个线程来并行`SIMD`的向量化计算过程。

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-102929.png" alt="image-20230316032928587" style="zoom:50%;" />

### Lantency & Bandwith

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-092422.png" alt="image-20230316022422678" style="zoom:33%;" />

越高的内存带宽不等价于更高的内存延迟：虽然一次性能够和内存交互更多的数据，但一次存取数据（`load/store`）需要花费的时间可能更长。

### Latency

有两种手段处理延迟：

- reduce latency
- hide latency

降低延迟的方法包括**使用多级缓存**，隐藏延迟的方法包括：

- prefetching
- multi-threading (interleave processing of multiple threads **on the same core** to hide stalls)

#### Multithreading

##### 如何理解硬件多线程

硬件多线程，顾名思义，是需要硬件支持的多线程（当然也需要OS的配合来实现），在程序中使用的“多线程”是软件多线程，我们可以把硬件多线程理解为软件多线程的实体（但这不意味着两者具有一对一的映射关系）。硬件级别多线程设计的存在能够让系统的线程执行效率更高：处理器会为线程执行准备几个`execution context`区块，并**由处理器决定当前执行哪个(硬件)线程，而非操作系统**。

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-22-000313.png" alt="image-20230321170312728" style="zoom: 50%;" />

如上图所示，当我们在程序中生成了两个线程后，**操作系统**会负责将`pthread`（`software threads`）映射到处理器的`thread execution contexts`（`hardware threads`）上，接下来就是**处理器**的事了。

##### 多线程的trade-off

当使用多线程时，存在几个trade-off: 

首先是当`interleave`多个线程时，单个线程的执行时间变长了，但是系统整体的吞吐量提高了：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-095901.png" alt="image-20230316025901256" style="zoom:33%;" />

其次是当拥有较多数量的线程时，意味着系统有较强的`hiding latency`的能力；当可分配的线程数量较少时，单独的线程会拥有更大的`work set`，也就意味着`cache`中可以分配给每个线程的内存更多了。

##### 多线程设计模式分类

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-21-002511.png" alt="image-20230320172511197" style="zoom: 33%;" />

多线程的设计方法可以被分成两类：

- **Interleaved Multi-threading (Temporal Multi-threading)**

这种多线程模式意味着在每一个时钟周期内，每一个核只执行一个线程，这种设计模式还可以划分成两种子类：

1. Coarse-grained Multi-threading

在这种模式下，每个线程会一直执行到发生`costly stall`的时候（比如`2nd cache miss`），对于`shortly stall`并不关心。（每个线程需要独立的`PC`, `register file`等硬件支持）

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-21-002739.png" alt="image-20230320172739063" style="zoom:33%;" />

2. Fine-grained Multi-threading

在这种模式下，多个线程使用`robin-round`等方法`interleave`各自的`instruction`，当遇到发生`stall`的线程则会直接跳过。在这种模式下，不会单单等到某一个线程发生`costly stall`时处理器才让另一个线程来接管他。（每个线程需要独立的`PC`, `register file`等硬件支持）

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-21-002806.png" alt="image-20230320172806526" style="zoom:33%;" />

- **Simultaneous Multi-threading (SMT)**

Intel的**Hyper-threading**就是基于`SMT`的典型设计之一。在该模式下，一个物理核被分为多个逻辑核，这意味着在同一个时钟信号内，系统允许处理器同时执行多个线程。（每个线程需要独立的`PC`, `register file`等硬件支持）

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-21-002831.png" alt="image-20230320172831208" style="zoom:33%;" />

在`SMT`模式下的多线程因为要在同一时刻，在同一个内核中执行多个指令，所以可以理解为一种基于`Superscalar`设计的扩展。但`SMT`和`Superscalar`并不一样，`Superscalar`机制由于不具备独立的`Execution Context`，所以只能从一个`instruction stream`中选择独立指令，但是`SMT`允许处理器从多个`instruction stream`中选择独立指令（这也是`Hyper-threading`想法产生的原因，即想要从同一个指令流中选择多个独立的指令并不容易，所以Intel想要借助多个线程来从多个指令流中选择独立指令）。这也意味着`SMT`和`Superscalar`可以同时使用（虽然Intel的`Superscalar`机制实际上是用来帮助实现`Hyper-threading`了），以`GTX 680`为例（`Warp`在这里表示`instruction stream`）：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-21-010736.png" alt="image-20230320180736901" style="zoom:50%;" />

需要注意的是，`SMT`具备从多个`instruction stream`中选择独立指令的能力并不意味着它一定要这么做，处理器会找到能够利用核心内所有执行单元的最佳方案，这种方案可能会从同一个线程中选择多个指令（this thread has sufficient ILP）同时执行，这会让处理器表现的像`interleaved multi-threading`一样。

当然，超线程的使用未必就比单线程的执行效果更好，因为多个线程共享一个`cache`可能会导致更多的`cache miss`。不仅如此，**在同一个`physical core`内，不管处理器提供了多少可能的硬件线程，他们的很多计算资源都是共享的**（这也是为什么即使在多核时代还是会在同一个核内设计多个逻辑核的原因----我们只需要对原有结构做不多的改动，就能够提高很大的性能指标，比如奔腾4增加`HT`技术只需要多花费5%的核心面积，就可以增加15-30%的多线程性能，而如果增加物理核心，增加多少性能，就至少要增加多少比例的核心数量）。而且还会有因为线程隔离不到位导致的线程安全问题...

所以，当操作系统想要将`pthread`映射到硬件线程上时，会尽量将同一个任务的两个线程（很大可能有资源的交互，即不完全独立）映射到两个不同的`physical core`内以确保最大可能的并发，之后再调度其他软件线程到`physical core`内剩下的`execution context`中执行。

### CPU & GPU

- CPU

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-100623.png" alt="image-20230316030623713" style="zoom:33%;" />

> 64=16*4
>
> 512=64*8

- CPU vs. GPU

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-16-101020.png" alt="image-20230316031020240" style="zoom:33%;" />

上图表明，CPU会利用更大的缓存来降低延迟，同时使用指令预抓取的方法隐藏延迟。而GPU则将更多的资源集中在了多线程上，它只配备了很小的缓存。进一步的计算可发现，想要完全利用GPU自身提供的多线程/SIMD/多核机制时很难的，换句话说我们需要花费很大的代价才能让GPU keep busy，其所需的`bandwith`远超上图的`177GB/sec`（一秒钟内转移多少数据才能让下一秒的GPU一直干活，如此循环往复）。这种现象的原因被称为**bandwith limited**。

## Lecture 3: Program Abstractions

### ISPC

**ISPC**实际上是`SPMD(Sing-Program-Multiple-Data)`模式的一种编程抽象。

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-26-022751.png" alt="image-20230325192751171" style="zoom:50%;" />

`ISPC`代码调用的时候会生成多个`program instances`，我们可以利用`programCount`和`programIndex`来获取在一个`gang`中的实例总数以及当前所在的实例。此外，`uniform`可以用作确保所有实例只使用变量的同一个副本。

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-26-023147.png" alt="image-20230325193146493" style="zoom:50%;" />

`ISPC`可以考虑两种实现方式，除了上图所示的方法外，我们还可以把代码写成如下的形式：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-26-023623.png" alt="image-20230325193622657" style="zoom:50%;" />

第一种方式要优于第二种，因为第二种方式想要利用`SIMD`指令进行加速必须先将不同位置的数值`gather`到一起，以下两幅图清楚的展现了上述两种方法的模型抽象：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-26-023834.png" alt="image-20230325193833803" style="zoom:50%;" />

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-26-023851.png" alt="image-20230325193851332" style="zoom:50%;" />

而在通常的`ISPC`编程中，可以直接利用`foreach (i = 0 ... N)`来实现（内部采用的是第一种方法），更加简洁。

总结一下，`ISPC`的代码中的`for`循环并没有被编译器显式`parallel`，只是在程序启动时生成的多个`program instances`会根据我们代码的逻辑选择如何执行以及执行哪一部分的循环（每个实例有不同的`ProgramIndex`），同时会利用`SIMD`指令对指令做隐式`parallel`。

##### 要注意的问题

这张图说明了可能存在的问题：

<img src="https://shaopu-blog.oss-cn-beijing.aliyuncs.com/img/2023-03-26-024717.png" alt="image-20230325194717294" style="zoom:50%;" />

以上的代码和这段`SIMD`指令效果相同：

```cpp
float sumall2(int N, float* x) {

  float tmp[8]; // assume 16-­‐byte alignment __mm256 partial = _mm256_broadcast_ss(0.0f);

  for (int i=0; i<N; i+=8)
  	partial = _mm256_add_ps(partial, _mm256_load_ps(&x[i]));

  _mm256_store_ps(tmp, partial);

  float sum = 0.f;
  for (int i=0; i<8; i++)
  	sum += tmp[i];
  return sum;
}
```

##### 多核ISPC

以上讨论的情况全部是单核中的`ISPC`模型，会使用`SIMD`进行指令加速。此外，`ISPC`还可以利用`tasks`来实现多核的指令加速（利用多线程）。

这里首先要注意的一个问题是，`ISPC`中的`tasks`并非等同于`thread`，当我们调整任务数量的时候会发现，在任务数量等同于CPU的逻辑线程数量的时候并非是加速比达到最大，如果继续增大任务数量，加速比会继续提高。

根据官方文档的说法，我们应该启动比逻辑线程更多的任务数量，以确保更好的负载均衡和性能。

> **Tasks are independent work that can be executed with different cores.** **Contrary to threads, they do not have execution context and they are only pieces of work**. The ISPC compiler takes the tasks and launches however many threads it decides.
>
> In general, one should launch many more tasks than there are processors in the system to ensure good load-balancing, but not so many that the overhead of scheduling and running tasks dominates the computation.

```cpp
// slightly different kernel to support tasking
task void mandelbrot_ispc_task(uniform float x0, uniform float y0, 
                               uniform float x1, uniform float y1,
                               uniform int width, uniform int height,
                               uniform int rowsPerTask,
                               uniform int maxIterations,
                               uniform int output[])
{

    // taskIndex is an ISPC built-in
    
    uniform int ystart = taskIndex * rowsPerTask;
    uniform int yend = ystart + rowsPerTask;
    
    uniform float dx = (x1 - x0) / width;
    uniform float dy = (y1 - y0) / height;
    
    foreach (j = ystart ... yend, i = 0 ... width) {
            float x = x0 + i * dx;
            float y = y0 + j * dy;
            
            int index = j * width + i;
            output[index] = mandel(x, y, maxIterations);
    }
}

export void mandelbrot_ispc_withtasks(uniform float x0, uniform float y0,
                                      uniform float x1, uniform float y1,
                                      uniform int width, uniform int height,
                                      uniform int maxIterations,
                                      uniform int output[])
{

    uniform int rowsPerTask = height / 25;

    // create 2 tasks
    launch[25] mandelbrot_ispc_task(x0, y0, x1, y1,
                                     width, height,
                                     rowsPerTask,
                                     maxIterations,
                                     output); 
}
```

> The launch keyword in `ISPC` provides a way for a "task" to execute asynchronously. These tasks can run concurrently on multiple cores, and there is no guarantee about the order in which they will execute, especially if there are more tasks launched than available threads. **Maybe a good way of thinking about a task is just a "thing to do" and threads are just workers that "do the things".**




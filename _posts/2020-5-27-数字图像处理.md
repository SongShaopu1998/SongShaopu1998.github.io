---
layout:		post
title:		数字图像处理
subtitle:	OpenCV基础应用(一)
date:		2020-5-27
author:		shaopu
header-img:	img/code.png
catalog:	true
tags:
    - Python
    - OpenCV
---

# 数字图像处理博客项目笔记（一）

Image processing code based on  **Adrian Rosebrock's** blogs.
- <https://www.pyimagesearch.com/>

我学习的代码可以前往[我的Github仓库](https://github.com/SongShaopu1998/Image_processing)查看。

本文章为第一部分。

## K-means

使用这种方法对颜色进行聚类，使用的是sklearn中的Kmeans方法，当设定聚类的数目在5以下时，能够取得比较好的效果；本程序的主要亮点在于对颜色直方图的显示，具体可参见代码utils.py.

> 这里的k-means只支持二维数组，所以先要把图片reshape一下

## Color-transfer

这里用到的主要是Reinhard等人在2001年发表的一篇论文里的色彩转换方法。

主要步骤如下：

1. 输入两张图片，一张是待处理图片-source，另一张是包含希望待处理图片模仿的色彩空间的图片-target
2. 将图片的色彩空间都转化到Lab空间中，Lab彩色空间模仿了人眼的感知均匀性，其中颜色值的微小变化也应该在颜色重要性上产生相对应的变化，它比RGB空间在模仿人类如何解释颜色上做得更好
3. 计算两张图片的每个Lab通道的平均值和方差
4. 将target每个通道中减去对应的Lab通道的平均值
5. 对target进行缩放，缩放因子为StdTAR/StdSRC*channel
6. 将source每个通道加上对应的Lab通道均值
7. 将通道合并在一起
8. 转换回RGB

## Color equalization using K-means

色彩量化是图像压缩的一个基本方法，常常使用聚类方法来操作，通过颜色聚类，选出具有代表性的单一颜色，从而大大减小图像的体积，同时根据设定的聚类数保留自己需要的颜色外观

MiniBatchKMeans要比普通的K-means块，但是颜色的中心可能不稳定，这类似于深度学习处理小的batch的思想

python中的k聚类只支持2维，所以要把图片reshape到二维，而之所以采用lab空间，是因为K-Means基于欧氏距离的特性，lab能够直接使用欧氏距离判定颜色关系，但是RGB不行

```python
quant = clt.cluster_centers_.astype("uint8")[labels]
```

这行关键的代码是从程序给出的颜色聚类质心集合中找对原始图片通过聚类预测的分类对应的簇中心坐标。
cluster_centers_ 是各个类别的聚类中心值，而之前的labels里存的是，比如k=16，那么就是对应
了0-15之间的类别标签，也就是predict函数给出的，（fit函数是得到一个类别的预测器，predict函数
把其中的labels列表给提取出来了）

## Hough circle transformation

霍夫圆变换的关键就在于那行函数的应用，注意它的返回值的格式是[1][(x,y)][R],所以可以用for循环对其进行遍历画图
注意最后后一个参数的选取含义，各个圆心之间的最小距离

## Super-pixels

超像素的含义是像素的集合，相当于构成了一个更大的像素块，它的最大的好处就是降维，提高了计算效率，
但是保持了图片中重要的分界线与纹理和颜色特征。生成的图片是一种“representationally efficient”，
一个实现超像素的方法就是简单线性迭代聚类分割，也就是常说的**SLIC**方法，这里使用skimage实现，注意先要把图片转化为浮点数形式来处理。

## Detect-color

这个程序的目标是检测几种颜色，首先规定颜色区域，并利用按位操作与掩膜将规定的颜色显示出来，注意：
inRange仅仅接受array的参数类型，所以先要从list转化为array，这个函数将区域内的所有值都
设置为255，区域之外都设置为0；bitwise_and函数按位操作，这里如果使用mask掩膜，那么一般的
调用格式就是这样，相当于按位操作的对象变成了mask数组和图片。

## Transformation

程序的核心是这两行代码：

```python
# 得到仿射变换矩阵

M = cv2.getPerspectiveTransform(rect, dst)

# 应用仿射变换矩阵

warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
```

同时在程序中也有一些值得注意的点：

因为在这份程序中，原图的角点是在一开始手动给出的，所以我们先要根据角点确定出矩形角点的顺序，也就是0123分别对应哪个，
这里作者使用的方法是通过对(x+y)与(x-y)的值判断；注意numpy的argmin/argmax函数返回的是索引，不是值；在确定了角点之后
我们就可以根据角点来确定长宽，从而给出目标的矩形坐标。

在example程序中，eval的作用是执行一个字符串表达式，并返回表达式的值。

## Scan

总的来说分为四步：

1. 边缘检测
2. 找到所需轮廓
3. 仿射变换 + 阈值分割

扫描的关键当然是边缘检测，这里利用Canny算子检测，在进行边缘检测之前，首先要将图像模糊以降噪，否则过多的图像噪声将会严重影响
边缘的分离度。考虑到cv的版本不同，函数grab_contours根据返回值的数目分离出我们需要的轮廓点集。

值得注意的是Python内置函数sorted在这里的使用方法，其中的cv2.contourArea表示多边形的面积，reverse=True表示降序排列，其实
这里的参数也是可调的，比如可以抽出前6个元素，看看效果比对如何。

argLength函数计算多边形周长，第二个参数True表示是封闭曲线，approxPolyDP用指定精度：0.02 * peri，来使用更少的点近似之前的多边形轮廓。

在这之后就用一个自适应阈值分割函数将图像二值化即可，是作者提供的包里的threshold_local.

>注： 一个array类型在放入数组括号内后就会转化为一个一维的list

## Threshold

基本函数cv2.threshold参数分别为：待处理图片、设定阈值、当图像像素值大于设定阈值时我们想要赋予的新值，阈值分割类型。

- cv2.THRESH_BINARY
物体被分割成黑色，背景为白色
- cv2.THRESH_BINARY_INV
物体为白色，背景为黑色
- cv2.THRESH_TRUNC
如果目标像素小于提供阈值那就保持不变
- cv2.THRESH_TOZERO
如果目标像素小于提供阈值那就把他们设置为0
- cv2.THRESH_TOZERO_INV
如果目标像素小于设定阈值那就设置为255（相当于上一种方法的反转）

## Compare

比较图像相似度常用的有两种方法，MSE与SSIM，MSE的计算可以用numpy处理很简单的实现，SSIM可以直接调用
skimage中的实现，skimage因为版本变更，原博客中的代码不再适用，所在的位置从measure移动到了metric，
在使用matplotlib显示图像的时候，代码也应当变为：
```python
plt.get_cmap('gray')
```

关于MSE，也即均方误差，它的取值在0-INF之间，0表示完全相似，越高表示相似度很低，它是一种全局的比对方式，这导致即使最后算出的
MSE的值比较高，也不一定就能够说明图像相似度较低。

但是SSIM就不一样，他考虑了更多的局部关系，取值在[-1,1]之间，与MSE相反，1表示完全相似。SSIM的鲁棒性更强，同时兼顾了
图像结构的变化，对图像结构进行了建模；与之相反，MSE仅仅考虑了感知上的变化。

就运行速度而言，SSIM的运行速度要比MSE慢（但是效果相对更好）

我们可以学习一下程序中for循环的使用方法。

## Bright

找到图片中的亮点要通过cv2.minMaxLoc函数来实现，它接受一个灰度图像的参数，并返回四个值，分别是：

- 灰度图像中最小像素强度值
- 最大像素强度值
- 指定的min点的(x,y)
- 指定的max点的(x,y)

但是这个函数只能找到最亮的点，我们需要获得最亮的区域，高频噪声可能会对这一过程产生较大影响。
所以先要使用高斯模糊来均衡图像像素，削减高频噪声，之后设定一个合适的半径，从而选择自己需要的
亮点**区域**。

```python
gray = cv2.GaussianBlur(gray, (args["radius"], args["radius"]), 0)
(minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(gray)
```

## Find_shapes

这个程序比较简短，为了找到图片中的黑色物体，其实只要设定好上界下界，再用inRange函数处理即可

## Detect_Barcode

算法的基本思路：
1. 使用Scharr算子计算x与y方向的梯度
2. 用x方向上的梯度减去y方向的梯度，从而获得较高的横向梯度与较低的纵向梯度(**Why?**)
3. 将图像模糊消除噪点后，将图像二值化
4. 使用形态学操作处理图片，先建立一个封闭内核(closing kernel)
5. 使用多次腐蚀、膨胀操作
6. 找到图片中最大的轮廓，就是二维码

在OpenCV中使用Sobel函数运算时，如果ksize为1，那么指定的是Scharr operation，这种方法在对3*3的内核操作时要比Sobel
有更好的效果。

下边一个我不理解的地方就是为什么用x方向上的梯度图减去y方向的梯度图可以获得较高的水平梯度和较小的垂直梯度，之后的convertScaleAbs
函数可以把数组中的元素计算绝对值并全部转化为uint8类型，之前之所以要转换为float类型是因为梯度的计算必定会产生一些负数，如果还是用
uint8，那将发生截断。

利用blur函数，是因为我们只想在图片中保留二维码部分，为了进一步提高图片处理效果，使用了多个形态学操作。
```python
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7))
```
getStructuringElement函数会返回指定形状和尺寸的结构元素，可以是CROSS，RECT或者ECLIPSE，这个内核由于隶属于形态学操作的范畴，而形态学操作
特指是针对于二值化图像的，那么这个内核中的元素也只会有0和1两种，我们称这个核为结构化元素，函数的命名也是这么来的,看一眼函数的原型：
```python
getStructuringElement(shape, ksize, anchor=None)
```
一般在腐蚀膨胀操作之前，会先使用这个函数来建立内核。

```python
closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
```
函数第二个参数为使用的形态学方法：
- MORPH_OPEN表示开运算，对图像先腐蚀再膨胀，可以排除小团物体
- MORPH_CLOSE表示闭运算，对图像先膨胀，再腐蚀，可以排除小型黑洞
- MORPH_GRADIENT返回图片为膨胀图与腐蚀图之差，可以保留物体的边缘轮廓
- MORPH_TOPHAT返回图像为原图像与开运算结果图之差
- MORPH_BLACKHAT返回图片为闭运算结果图与原图像之差

```python
morphologyEx(src, op, kernel, dst=None, anchor=None, iterations=None, borderType=None, borderValue=None)
```
因为我们的目标是去除当前的二值化图像中的小空隙(small blobs)，所以当然使用MORPH_CLOSING

关于接下来的腐蚀膨胀操作，腐蚀是把前景中的白色像素腐蚀掉，也就是变为0，卷积核正是在这一过程中体现，前景物体会变小，整幅图像的白色区域会减少，这对于去除
白噪声很有用。正是因为腐蚀把图像缩小，那我们可以利用膨胀操作来把保留的区域变大。

再这个项目中，腐蚀的作用是去掉二维码之外的白色小块，膨胀则将继续生长二维码白色区域。
设置迭代4次。

在下边的轮廓查找函数中，我们只选取了最大的轮廓边缘。
```python
cnts = cv2.findContours(closed.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
```
第二个参数：
- cv2.RETR_EXTERNAL表示只检测外轮廓
- cv2.RETR_LIST检测的轮廓不建立等级关系
- cv2.RETR_CCOMP建立两个等级的轮廓，上面的一层为外边界，里面的一层为内孔的边界信息。如果内孔内还有一个连通物体，这个物体的边界也在顶层
- cv2.RETR_TREE建立一个等级树结构的轮廓

第三个参数：
- cv2.CHAIN_APPROX_NONE存储所有的轮廓点，相邻的两个点的像素位置差不超过1
- cv2.CHAIN_APPROX_SIMPLE压缩水平方向，垂直方向，对角线方向的元素，只保留该方向的终点坐标

之后找到能够包含输入点集的有最小面积的矩形轮廓：rect中包含矩形中心点坐标，边界信息等等
```python
rect = cv2.minAreaRect(c)
```
于是使用boxpoints函数获取四个角点坐标。

我们可以看一下cv2.minAreaRect的返回值：
- rect[0][0] + rect[0][1] == 矩形中心点坐标(x,y)
- rect[1][0] + rect[1][1] == (width, height)
- rect[2] == 旋转角度

## Video_barcode_detection

视频的处理被分成两种情况，第一种是通过摄像头读取实时视频，第二种是直接使用本地存储的视频序列。

如果是调用摄像头，那么每一个frame对应一帧，它的结构就是简单的图片形式，但是如果调用的是存储视频，那么frame的格式
通过调试可以看到是一个tuple，tuple[0]是一个bool值，所以我们要选取frame[1]；

在之后的按键中断部分，
```python
cv2.imshow("Frame", frame)
key = cv2.waitKey(1) & 0xFF

if key == ord("q"):
    break
```
waitKey函数的返回值在没有按键时为-1，否则返回按键对应的ASCII值，
-1对应的十六进制为0xFFFF,二进制全部为1。

Python内置函数ord参数是一个Unicode字符，返回对应的整数，另一个内置函数chr函数与之相反，接受一个整数，返回对应的ASCII值。
正是考虑到ord函数的返回值是一个整数的这一特点，我们对之前waitKey函数的返回值与255的十六进制表示0xFF进行了按位与操作，这样
整数的范围就被限制在了0-255之间，并可以直接进行与ord返回值（均为整数）的比较。

## Distance_to_camera

利用三角形相似计算物体距离，整个流程应当分为两步：
1. 相机焦距=(物体与相机当前距离*物体在视图中的宽度) / 物体的真实高度
2. 移动后的物体与相机距离=(相机焦距*物体的真实高度) / 物体在视图中的宽度

为了求出物体在视图中的宽度，我们需要用矩形框(或者在其他的应用中对应的边界)框出物体，走边缘检测的流程(先高斯模糊再Canny算子检测)，
之后只需要代公式即可，在程序的最后有一个函数值得注意：
```python
cv2.putText(image, "%.2fft" % (inches / 12), (image.shape[1] - 200, image.sjape[0] - 20),
            cv2.FONT_HERSHEY_SIMPLEX, 2.0, (0, 255, 0), 3)
```
其中的第三个参数意思是我们想要在图片右下角标注的位置，也就是注释框中心点坐标，其后的一个参数是字体格式。

## Match

OpenCV中内置的模板匹配函数cv2.matchTemplate
应用的效果往往不尽如人意，检测出来的匹配区域过小。所以在大多数场景下，使用SIFT或者SURF算子来做模板匹配，
但是作者提供了一种可以较好的处理方法，即通过不断地调整图像的缩放比例来确定匹配度最高
的情况（图像大小）；但是这种方法也存在缺点，比如在图像存在仿射变换或者很明显的旋转关系
时不能够奏效，那时我们还是需要选择SIFT或者SURF、ORB等方法来做模板匹配。

在进行模板匹配之前，注意先将图片准化成Canny算子做完**边缘检测**的情况，**根据实际检验，边缘图像做模板匹配的
效果要好于RGB或者灰度图**。

为了实现不同尺度的缩放，进行如下操作：
```python
for scale in np.linspace(0.2, 1.0, 20)[::-1]:
```
有点像MATLAB，三个参数分别是起始点，终止点，切片数；同时设定最小条件，在缩放过小时自动跳出循环；

接下来是关键的模板匹配算法代码：
```python
result = cv2.matchTemplate(edged, template, cv2.TM_CCOEFF)
(_, maxVal, _, maxLoc) = cv2.minMaxLoc(result)
```
对于函数matchTemplate，有如下分析：
- CV_TM_SQDIFF  平方差匹配法：该方法采用平方差来进行匹配；最好的匹配值为0；匹配越差，匹配值越大
- CV_TM_CCORR   相关匹配法：该方法采用乘法操作；数值越大表明匹配程度越好
- CV_TM_CCOEFF  相关系数匹配法：1表示完美的匹配；-1表示最差的匹配
- CV_TM_SQDIFF_NORMED   计算归一化平方差，计算出来的值越接近0，越相关
- CV_TM_CCORR_NORMED    计算归一化相关性，计算出来的值越接近1，越相关
- CV_TM_CCOEFF_NORMED	计算归一化相关系数，计算出来的值越接近1，越相关

他常常与之后的函数：minMaxLoc配套使用，因为模板匹配函数本身返回的是一个矩阵，这个矩阵包含了图片各个位置的匹配度，我们一般利用其中的最高点（因为
匹配度最高的点一般就是模板匹配的起始点，可以把图片压成一个一维数组来想），而minMaxLoc正可以返回最值和最值的索引。

于是我们可以一直更新最大相似度的最大值，并将其保存下来。

在得到初步的匹配位置之后，我们尝试画图，这里有一个函数：
```python
clone = np.dstack([edged, edged, edged])
```
这个函数的作用类似于concatenate(src,axis=2)

把提供数组的每一维对应的位置拼在一起，那这里其实就是把灰度图转化成为了一个三通道的图片格式，方便最后的作图工作。
在绘图时，不要忘记将缩放的比例乘上，因为我们最终还是要在原图上绘图，而不是缩放之后的图片。

## Remove_contours

轮廓去除的关键在于掩膜的选择方式，作者选用了所有不是矩形的轮廓，在制作掩膜时记得参数thickness设置为-1，因为中间的区域也要画上，之后利用
bitwise_and进行按位与操作即可。

## Click_and_crop

本程序实现了一个简单的截图操作，并在源程序的基础上进行了改进，使之能够看到鼠标的实时拖动情况，OpenCV中的鼠标检测靠回调函数实现：
```python
cv2.setMouseCallback("image", click_and_crop)
```
第一个参数是窗口的名称，第二个参数是回调响应函数指针，基本的检测思路是：
**在一个while循环内：**
- 正常情况下显示原图片
- 如果鼠标点下并开始拖动，那我们实时显示矩形框的拖动位置；
- 在鼠标弹起后，这一过程结束；
- 我们可以重新选择(R)或者跳出这个循环显示裁剪图片(C)

那余下的部分就是回调响应函数的设计，鼠标检测可以有三个状态，按下(DOWN),弹起(UP)和移动(MOVE),我们需要确定这三个
动作应该对应到什么状态。**回调响应函数应当接受五个参数**：
1. event
2. x
3. y
4. flag(由OpenCV传递)
5. params(一个OpenCV传递的外部参数)

- 如果鼠标按下，那说明我们要开始拖动了，并且应该记录下起始位置点；
- 如果鼠标松开(弹起)，那说明拖动结束了，我们要记录终止位置点，**同时把状态变量还原**，并且显示最终位置；
- 最后为了实时显示拖动状态，我们还需要一个实时记录矩形右下角点的list，把这个过程对应到之前的
while循环中，就可以实时显示矩形位置。

## Sorting_contours

根据轮廓进行物体排序在实际中有十分重要的应用，这个程序提供了两种排序方式，分别是根据轮廓面积排序和根据位置排布排序；
在根据位置排布排序中，又分为左右排序和上下排序，他们分别对应了两个标志：i与reverse；反别决定了是使用x/y排序和使用升序/降序排序；
而利用zip+sorted+lambda的联合使用方法，仅仅用了一行代码就解决了这个排序问题（这行代码意义何在在别的专题有详细阐述）。
```python
(cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][i], reverse=reverse))
```

之后比较重要的一点就是如何在一个矩形不是完美的与x/y轴平行时，计算矩形的质心，这就用到了OpenCV的moment函数：
```python
M = cv2.moments(c)
cX = int(M["m10"] / M["m00"])
cY = int(M["m01"] / M["m00"])
```
依靠这三行代码，我们就可以计算出举行的质心，moment函数可以算出特征矩，这可不是随便算出来的，是根据严格的数学推导得到的，这里公式
就不具体展开了，不然显然离主题有点远，可以自行上网查看。

在之后根据轮廓面积排序的代码中，作者将图片的通道拆分，分别进行了中值滤波与边缘检测，并在最后通过累计的方式（用按位与）将边缘图片merge在一起，以获得
更好的检测效果。
```python
for chan in cv2.split(image):
    chan = cv2.medianBlur(chan, 11)
    edged = cv2.Canny(chan, 50, 200)
    accumEdged = cv2.bitwise_or(accumEdged, edged)
```